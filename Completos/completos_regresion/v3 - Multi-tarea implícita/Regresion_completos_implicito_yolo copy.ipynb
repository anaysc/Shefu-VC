{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd48c411",
   "metadata": {},
   "source": [
    "# Shefu — YOLOv8 + Regresión Implícita (imagen → nota; criterios aprendidos internamente)\n",
    "\n",
    "Este cuaderno entrena un modelo **de una sola salida (nota 0–100)** que aprende internamente los 4 criterios visuales mediante una **rama auxiliar** usada solo en entrenamiento.\n",
    "\n",
    "**Pipeline:** YOLOv8 → crop → red con pérdida auxiliar → salida nota.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics==8.2.103 tensorflow==2.15.0 pandas==2.1.4 numpy==1.26.4 scikit-learn==1.3.2 matplotlib==3.8.0\n",
    "from ultralytics import YOLO\n",
    "import os, json, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "print('TensorFlow:', tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ecf22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CSV_PATH = '/mnt/data/project-2-at-2025-09-12-06-46-b36c7e7f.csv'\n",
    "IMAGES_ROOT  = '/mnt/data/images'\n",
    "YOLO_WEIGHTS = r'C:\\\\ruta\\\\a\\\\tu\\\\runs_detector1\\\\rf_completos22\\\\weights\\\\best.pt'\n",
    "CROPS_DIR    = '/mnt/data/crops_yolo'\n",
    "IMG_SIZE     = 224\n",
    "BATCH_SIZE   = 16\n",
    "EPOCHS       = 25\n",
    "VAL_SPLIT    = 0.2\n",
    "RANDOM_STATE = 42\n",
    "os.makedirs(CROPS_DIR, exist_ok=True)\n",
    "print('CSV:', DATA_CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_score_field(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    try:\n",
    "        data = json.loads(x)\n",
    "        if isinstance(data, list) and len(data) > 0 and 'number' in data[0]:\n",
    "            return float(data[0]['number'])\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        data = ast.literal_eval(x)\n",
    "        if isinstance(data, list) and len(data) > 0 and 'number' in data[0]:\n",
    "            return float(data[0]['number'])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.nan\n",
    "\n",
    "def parse_motivo_to_flags(motivo):\n",
    "    flags = {'pan_quemado':0, 'falta_ingrediente':0, 'desordenado':0, 'buen_balance_visual':0}\n",
    "    if pd.isna(motivo):\n",
    "        return flags\n",
    "    text = str(motivo)\n",
    "    choices = None\n",
    "    if '\"choices\"' in text or \"'choices'\" in text:\n",
    "        try:\n",
    "            obj = json.loads(text)\n",
    "        except Exception:\n",
    "            try:\n",
    "                obj = ast.literal_eval(text)\n",
    "            except Exception:\n",
    "                obj = None\n",
    "        if isinstance(obj, dict) and 'choices' in obj:\n",
    "            choices = obj['choices']\n",
    "    def set_from(s):\n",
    "        s = s.lower()\n",
    "        if 'pan quemado' in s: flags['pan_quemado']=1\n",
    "        if 'falta de ingrediente' in s: flags['falta_ingrediente']=1\n",
    "        if 'desordenado' in s: flags['desordenado']=1\n",
    "        if 'buen balance visual' in s: flags['buen_balance_visual']=1\n",
    "    if choices is None:\n",
    "        set_from(text)\n",
    "    else:\n",
    "        for c in choices:\n",
    "            set_from(str(c))\n",
    "    return flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9930eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = YOLO(YOLO_WEIGHTS)\n",
    "def yolo_crop_to_file(img_path, save_dir=CROPS_DIR, conf=0.25):\n",
    "    base = os.path.basename(img_path)\n",
    "    crop_path = os.path.join(save_dir, base)\n",
    "    if os.path.exists(crop_path):\n",
    "        return crop_path\n",
    "    results = detector(img_path, conf=conf, verbose=False)\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy().astype(int) if len(results)>0 else np.array([])\n",
    "    if boxes.shape[0]==0:\n",
    "        return None\n",
    "    x1,y1,x2,y2 = boxes[0]\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    h,w = img.shape[:2]\n",
    "    x1,y1 = max(0,x1), max(0,y1)\n",
    "    x2,y2 = min(w,x2), min(h,y2)\n",
    "    crop = img[y1:y2, x1:x2]\n",
    "    if crop.size==0:\n",
    "        return None\n",
    "    cv2.imwrite(crop_path, crop)\n",
    "    return crop_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc9cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_CSV_PATH)\n",
    "df['score_clean'] = df['score'].apply(parse_score_field)\n",
    "flags_series = df['motivo'].apply(parse_motivo_to_flags)\n",
    "for k in ['pan_quemado','falta_ingrediente','desordenado','buen_balance_visual']:\n",
    "    df[k] = flags_series.apply(lambda d: d[k])\n",
    "def resolve_original_path(p):\n",
    "    if pd.isna(p): return None\n",
    "    base = os.path.basename(str(p))\n",
    "    return os.path.join(IMAGES_ROOT, base)\n",
    "df['orig_path'] = df['data'].apply(resolve_original_path)\n",
    "crop_paths = []\n",
    "for p in df['orig_path'].tolist():\n",
    "    if p is None or not os.path.exists(p):\n",
    "        crop_paths.append(None); continue\n",
    "    cp = yolo_crop_to_file(p, save_dir=CROPS_DIR, conf=0.25)\n",
    "    crop_paths.append(cp)\n",
    "df['crop_path'] = crop_paths\n",
    "df = df.dropna(subset=['score_clean','crop_path']).reset_index(drop=True)\n",
    "df = df[df['crop_path'].apply(lambda p: os.path.exists(p))].reset_index(drop=True)\n",
    "print('Total con crop válido:', len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06465592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "IMG_SIZE=224\n",
    "BATCH_SIZE=16\n",
    "VAL_SPLIT=0.2\n",
    "RANDOM_STATE=42\n",
    "train_df, val_df = train_test_split(df, test_size=VAL_SPLIT, random_state=RANDOM_STATE, shuffle=True)\n",
    "BIN_COLS = ['pan_quemado','falta_ingrediente','desordenado','buen_balance_visual']\n",
    "def load_and_resize(p):\n",
    "    img = tf.io.read_file(p)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img = tf.cast(img, tf.float32)/255.0\n",
    "    return img\n",
    "def augment(img):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_brightness(img, 0.1)\n",
    "    img = tf.image.random_contrast(img, 0.9, 1.1)\n",
    "    return img\n",
    "def make_ds(frame, bs, training=True):\n",
    "    xi = frame['crop_path'].values\n",
    "    ys = (frame['score_clean'].values/100.0).astype(np.float32)\n",
    "    yf = frame[BIN_COLS].values.astype(np.float32)\n",
    "    ds_img = tf.data.Dataset.from_tensor_slices(xi).map(load_and_resize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if training:\n",
    "        ds_img = ds_img.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds_y_s = tf.data.Dataset.from_tensor_slices(ys)\n",
    "    ds_y_f = tf.data.Dataset.from_tensor_slices(yf)\n",
    "    ds_y = tf.data.Dataset.zip((ds_y_s, ds_y_f)).map(lambda s,f: {'score':s, 'flags':f})\n",
    "    ds = tf.data.Dataset.zip((ds_img, ds_y))\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=len(frame), reshuffle_each_iteration=True)\n",
    "    return ds.batch(bs).prefetch(tf.data.AUTOTUNE)\n",
    "train_ds = make_ds(train_df, 16, True)\n",
    "val_ds   = make_ds(val_df, 16, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='img')\n",
    "bb = MobileNetV3Small(include_top=False, weights='imagenet', input_tensor=inp)\n",
    "bb.trainable = False\n",
    "x = layers.GlobalAveragePooling2D()(bb.output)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "aux = layers.Dense(32, activation='relu')(x)\n",
    "out_flags = layers.Dense(4, activation='sigmoid', name='flags')(aux)\n",
    "h = layers.Concatenate()([x, out_flags])\n",
    "h = layers.Dense(64, activation='relu')(h)\n",
    "h = layers.Dropout(0.3)(h)\n",
    "out_score = layers.Dense(1, activation='linear', name='score')(h)\n",
    "model = models.Model(inputs=inp, outputs=[out_score, out_flags])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss={'score':'mse','flags':'binary_crossentropy'}, loss_weights={'score':1.0,'flags':0.3}, metrics={'score':['mae']})\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da752d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=[\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_score_loss', patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_score_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "h1 = model.fit(train_ds, validation_data=val_ds, epochs=25, callbacks=callbacks)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(); plt.plot(h1.history['score_loss']); plt.plot(h1.history['val_score_loss']); plt.title('MSE nota'); plt.legend(['train','val']); plt.show()\n",
    "plt.figure(); plt.plot(h1.history['score_mae']); plt.plot(h1.history['val_score_mae']); plt.title('MAE nota'); plt.legend(['train','val']); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=20\n",
    "for layer in bb.layers[-N:]: layer.trainable=True\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss={'score':'mse','flags':'binary_crossentropy'}, loss_weights={'score':1.0,'flags':0.3}, metrics={'score':['mae']})\n",
    "h2 = model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=callbacks)\n",
    "plt.figure(); plt.plot(h2.history['score_loss']); plt.plot(h2.history['val_score_loss']); plt.title('MSE nota FT'); plt.legend(['train','val']); plt.show()\n",
    "plt.figure(); plt.plot(h2.history['score_mae']); plt.plot(h2.history['val_score_mae']); plt.title('MAE nota FT'); plt.legend(['train','val']); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR='/mnt/data'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "full_path=os.path.join(OUTPUT_DIR,'shefu_impl_full_yolo.h5')\n",
    "model.save(full_path); print('Guardado full:', full_path)\n",
    "infer_model=tf.keras.Model(inputs=model.input, outputs=model.get_layer('score').output)\n",
    "infer_path=os.path.join(OUTPUT_DIR,'shefu_impl_infer_yolo.h5')\n",
    "infer_model.save(infer_path); print('Guardado infer:', infer_path)\n",
    "converter=tf.lite.TFLiteConverter.from_keras_model(infer_model)\n",
    "converter.optimizations=[tf.lite.Optimize.DEFAULT]\n",
    "tflite_bytes=converter.convert()\n",
    "tflite_path=os.path.join(OUTPUT_DIR,'shefu_impl_infer_yolo.tflite')\n",
    "with open(tflite_path,'wb') as f: f.write(tflite_bytes)\n",
    "print('Guardado TFLite:', tflite_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e9669",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    infer_model\n",
    "except NameError:\n",
    "    infer_model=tf.keras.models.load_model('/mnt/data/shefu_impl_infer_yolo.h5')\n",
    "def infer_full_image(image_path, conf=0.25):\n",
    "    cp=yolo_crop_to_file(image_path, save_dir=CROPS_DIR, conf=conf)\n",
    "    if cp is None or not os.path.exists(cp):\n",
    "        print('No se pudo obtener crop del completo.'); return None\n",
    "    img=tf.image.decode_jpeg(tf.io.read_file(cp), channels=3)\n",
    "    img=tf.image.resize(img,(IMG_SIZE,IMG_SIZE))\n",
    "    img=tf.cast(img,tf.float32)/255.0\n",
    "    img=tf.expand_dims(img,0)\n",
    "    pred=infer_model.predict(img,verbose=0)[0,0]\n",
    "    score=float(np.clip(pred,0,1)*100.0)\n",
    "    print(f'Nota estimada: {score:.1f}/100')\n",
    "    return score\n",
    "# Ejemplo: infer_full_image(r'C:\\\\ruta\\\\a\\\\una\\\\foto.jpg')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
